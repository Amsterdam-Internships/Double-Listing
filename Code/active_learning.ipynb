{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "\n",
    "from modAL.models import ActiveLearner\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import sklearn.linear_model\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "import xgboost as xgb\n",
    "\n",
    "\n",
    "import recordlinkage as rl\n",
    "\n",
    "from active_learn import*\n",
    "from thresholding import*\n",
    "\n",
    "from IPython import display\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "from importlib import reload\n",
    "#reload(spam)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_uns = pd.read_csv(r'C:\\Users\\Invitado\\Documents\\Python\\DS_MASTER\\City_Adam_Intern\\df_uns_label_img_1.csv')\n",
    "df_uns_red =pd.read_csv(r'C:\\Users\\Invitado\\Documents\\Python\\DS_MASTER\\City_Adam_Intern\\df_uns_label_img_red_hot.csv')\n",
    "\n",
    "df_full = pd.read_csv(r'C:\\Users\\Invitado\\Documents\\Python\\DS_MASTER\\City_Adam_Intern\\merged_clean_df.csv')\n",
    "df_full.drop('Unnamed: 0',axis=1,inplace=True)\n",
    "\n",
    "#Manually annotated data\n",
    "test_1 = rl.read_annotation_file(r'C:\\Users\\Invitado\\Documents\\Python\\DS_MASTER\\City_Adam_Intern\\result_bnb_trip.json')\n",
    "test_2 = rl.read_annotation_file(r'C:\\Users\\Invitado\\Documents\\Python\\DS_MASTER\\City_Adam_Intern\\result_bnb_vrb.json')\n",
    "\n",
    "#Baseline with unsupervised\n",
    "df_full_base = pd.read_csv(r'C:\\Users\\Invitado\\Documents\\Python\\DS_MASTER\\City_Adam_Intern\\df_uns_label_base_train.csv')\n",
    "df_full_base['label'] = df_full_base['label'].map(to_bin)\n",
    "\n",
    "# Updated training df\n",
    "#df_ams_train = pd.read_csv(r'C:\\Users\\Invitado\\Documents\\Python\\DS_MASTER\\City_Adam_Intern\\df_ams_train_1.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Divide test and training set for baseline \n",
    "df_base_train = df_full_base.loc[:1761] \n",
    "df_base_test = df_full_base.loc[1762:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compare(df_full,10161202, 10161074,full=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Labeled data\n",
    "open_file = open(r'C:\\Users\\Invitado\\Documents\\Python\\DS_MASTER\\City_Adam_Intern\\Double-Listing\\Data\\labels_no_hotels_v.pkl', \"rb\")\n",
    "labels_test = pickle.load(open_file)\n",
    "open_file.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Take out the labeled data for making a test set \n",
    "manual_labeled = []\n",
    "\n",
    "for (tuple_1,tuple_2),label in labels_test:\n",
    "    if label ==2 :\n",
    "        manual_labeled.append([str((tuple_1,tuple_2)),np.float('nan')])\n",
    "        manual_labeled.append([str((tuple_2,tuple_1)),np.float('nan')])\n",
    "    else:    \n",
    "        manual_labeled.append([str((tuple_1,tuple_2)),label])\n",
    "        manual_labeled.append([str((tuple_2,tuple_1)),label])\n",
    "\n",
    " #Merging to have it with the full data   \n",
    "df_labeled = pd.DataFrame(manual_labeled,columns=['ids','labels'])\n",
    "df_labeled\n",
    "with_labels =df_uns_red.merge(df_labeled,on='ids',how='left')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Comitee of models\n",
    "def randomforest_com():\n",
    "    clf = RandomForestClassifier()\n",
    "    return clf\n",
    "def desiciontree():\n",
    "    clf= DecisionTreeClassifier()#criterion ='gini', max_depth= None,max_leaf_nodes= 7,min_samples_leaf= 3\n",
    "    return clf\n",
    "def logisticreg():\n",
    "    clf = LogisticRegression()#fit_intercept= True,max_iter=150, penalty= 'l1',solver= 'saga'\n",
    "    return clf\n",
    "def xbg_class():\n",
    "    clf = xgb.XGBClassifier() #learning_rate=0.1,max_depth= 7,n_estimators = 150\n",
    "    return clf\n",
    "def svm():\n",
    "    clf = SVC(probability=True)#,C= 1,gamma= 0.1, kernel= 'linear'\n",
    "    return clf\n",
    "#Main model\n",
    "def randomforest_main():\n",
    "    clf = RandomForestClassifier(n_estimators=10,min_samples_split=2,warm_start=True,max_depth=5)\n",
    "    return clf\n",
    "\n",
    "list_models = [randomforest_com,desiciontree,logisticreg,xbg_class,svm,randomforest_main]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "users = [\"Host_first_name_jaro_winker\", \"Host_first_name_levenshtein_sim\" , \"Host_first_name_jaccard_sim\",\n",
    "        \"Host_first_name_relaxed_jaccard_sim\", \"Host_first_name_overlap_sim\", \"Host_first_name_containment_sim\",\n",
    "        'Host_picture_url_jaro_winker', 'Host_picture_url_levenshtein_sim',\n",
    "        'Host_picture_url_jaccard_sim', 'Host_picture_url_relaxed_jaccard_sim',\n",
    "        'Host_picture_url_overlap_sim', 'Host_picture_url_containment_sim']\n",
    "\n",
    "df_ams_train =pd.read_csv(r'C:\\Users\\Invitado\\Documents\\Python\\DS_MASTER\\City_Adam_Intern\\df_ams_train_nh_1.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add labels to test set \n",
    "df_ams_test = df_uns_red.merge(df_labeled,on='ids',how='inner').dropna()\n",
    "#Only run the rest of the code when starting from scratch otherwise just use updated df\n",
    "#df_ams_train = with_labels[with_labels.labels.isna()]\n",
    "df_ams_train.rename(columns={'labels':'label'},inplace=True)\n",
    "df_ams_test.rename(columns={'labels':'label'},inplace=True)\n",
    "#df_ams_train.drop(users,axis=1,inplace=True)\n",
    "df_ams_test.drop(users,axis=1,inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Recalculate the uns scores after taking out the test set\n",
    "df_ams_train =df_ams_train.drop(['agg_score','weights','uns_label'],axis=1)\n",
    "sorted_dataset = scoring_unsupervised(df_ams_train,baseline=False)\n",
    "threshold,index = elbow_threshold(sorted_dataset) \n",
    "matches_score_weight, nonmatches_score_weight = unsupervised_labels(sorted_dataset,threshold)\n",
    "columns = ['ids','agg_score','weights','uns_label']\n",
    "uns_labels = matches_score_weight+nonmatches_score_weight\n",
    "df_uns = pd.DataFrame(uns_labels,columns=columns)\n",
    "df_ams_train= df_ams_train.merge(df_uns,on='ids')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def al(df,df_test,queries = 20,baseline=False,committee_pred=False,incremental_comitee = False,boostrap = False,warm =False):\n",
    "\n",
    "    if incremental_comitee == True:\n",
    "        #Commitee of models\n",
    "        members = [randomforest_main,desiciontree,logisticreg,xbg_class,svm]\n",
    "    if incremental_comitee == False:\n",
    "         members = [randomforest_com,desiciontree,logisticreg,xbg_class,svm]\n",
    "\n",
    "    #List with comitee object\n",
    "    learner_list = list()\n",
    "\n",
    "    if baseline == False:\n",
    "        #Only use the data corresponding to the features to train\n",
    "        not_train = ['ListingId_1','ListingId_2','ids', 'agg_score', 'weights','uns_label','label']\n",
    "    if baseline == True:\n",
    "        #Only use the data corresponding to the features to train\n",
    "        not_train = ['source_id','target_id','ids', 'agg_score', 'weights','uns_label','label']\n",
    "\n",
    "    ids = df['ids'].values\n",
    "    X_pool = df.drop(not_train, axis=1).values\n",
    "    y_uns_label = df['uns_label'].values\n",
    "    y_uns_weight = df['weights'].values\n",
    "\n",
    "    y_pool_labels = df['label'].values\n",
    "    y_test = df_test['label'].values\n",
    "    X_test = df_test.drop(not_train, axis=1).values\n",
    "\n",
    "    #Will save the new labels being labeled\n",
    "    new_labels = []\n",
    "\n",
    "    if boostrap == True:\n",
    "        # Selecting the 2 most confident labels, a positive a negative with weight 1\n",
    "        train_idx = np.where(y_uns_weight> 0.99998)\n",
    "        #This is the labeled pool it starts with 2 examples\n",
    "        X_train_lb = X_pool[train_idx]\n",
    "        y_train_lb = y_uns_label[train_idx]\n",
    "        #Delete used results\n",
    "        X_pool = np.delete(X_pool, train_idx, axis=0)\n",
    "        ids = np.delete(ids, train_idx, axis=0)\n",
    "        y_uns_label = np.delete(y_uns_label, train_idx, axis=0)\n",
    "        y_uns_weight = np.delete(y_uns_weight, train_idx, axis=0)\n",
    "        y_pool_labels = np.delete(y_pool_labels, train_idx, axis=0)\n",
    "    else:\n",
    "\n",
    "        #If no boostrapping the model is initialized with random instances until there is one positive and negative\n",
    "        train_idx = np.random.choice(X_pool.shape[0], 1, replace=False)\n",
    "        X_train_lb = X_pool[train_idx]\n",
    "        if baseline ==True:\n",
    "                y_train_lb = y_pool_labels[train_idx]\n",
    "        else:\n",
    "            #\n",
    "            if np.isnan(y_pool_labels[train_idx]):\n",
    "                print(\"Are this two listings the same one? 1-match , 0-nonmatch, 2-more=info\")\n",
    "                ids_int = [int(x) for x in ids[train_idx][0][1:-1].split(',')]\n",
    "                print(compare(df_full,ids_int[0],ids_int[1]))\n",
    "                label = int(input())\n",
    "                if label == 2:\n",
    "                    print(compare(df_full,ids_int[0],ids_int[1],full=True))\n",
    "                    label = int(input())\n",
    "                    y_new = np.array([label], dtype=float)\n",
    "                else:\n",
    "                    y_new = np.array([label], dtype=float)\n",
    "                new_labels.append([(ids_int[0],ids_int[1]),label])\n",
    "                y_train_lb = y_new\n",
    "            else:\n",
    "                y_train_lb = y_pool_labels[train_idx]\n",
    "\n",
    "\n",
    "        #Delete instances moved\n",
    "        X_pool = np.delete(X_pool, train_idx, axis=0)\n",
    "        ids = np.delete(ids, train_idx, axis=0)\n",
    "        y_uns_label = np.delete(y_uns_label, train_idx, axis=0)\n",
    "        y_uns_weight = np.delete(y_uns_weight, train_idx, axis=0)\n",
    "        y_pool_labels = np.delete(y_pool_labels, train_idx, axis=0)\n",
    "\n",
    "\n",
    "        while 1 not in y_train_lb or 0 not in y_train_lb :\n",
    "            # Keep selecting till at least 1 postive and neagtive label has been added\n",
    "            train_idx = np.random.choice(X_pool.shape[0], 1, replace=False)\n",
    "            X_train_lb = np.concatenate((X_train_lb,X_pool[train_idx]),axis=0)\n",
    "            #For baseline just take the correct label\n",
    "            if baseline ==True:\n",
    "                y_train_lb = np.concatenate((y_train_lb,y_pool_labels[train_idx]),axis =0)\n",
    "            else:\n",
    "                #If label not available ask for it to the oracle\n",
    "                if np.isnan(y_pool_labels[train_idx]):\n",
    "                    print(\"Are this two listings the same one? 1-match , 0-nonmatch, 2-more=info\")\n",
    "                    ids_int = [int(x) for x in ids[train_idx][0][1:-1].split(',')]\n",
    "                    print(compare(df_full,ids_int[0],ids_int[1]))\n",
    "                    label = int(input())\n",
    "                    #Extra info needed\n",
    "                    if label == 2:\n",
    "                        print(compare(df_full,ids_int[0],ids_int[1],full=True))\n",
    "                        label = int(input())\n",
    "                        y_new = np.array([label], dtype=float)\n",
    "                    else:\n",
    "                        y_new = np.array([label], dtype=float)\n",
    "                    new_labels.append([(ids_int[0],ids_int[1]),label])\n",
    "                    y_train_lb = np.concatenate((y_train_lb,y_new),axis =0)\n",
    "                #If label has already been given in past runs just take it\n",
    "                else:\n",
    "                    y_train_lb = np.concatenate((y_train_lb,y_pool_labels[train_idx]),axis =0)\n",
    "\n",
    "            #Delete from unlabeled pool\n",
    "            X_pool = np.delete(X_pool, train_idx, axis=0)\n",
    "            ids = np.delete(ids, train_idx, axis=0)\n",
    "            y_uns_label = np.delete(y_uns_label, train_idx, axis=0)\n",
    "            y_uns_weight = np.delete(y_uns_weight, train_idx, axis=0)\n",
    "            y_pool_labels = np.delete(y_pool_labels, train_idx, axis=0)\n",
    "\n",
    "        #Check proggress\n",
    "        print(X_train_lb.shape,y_train_lb.shape)\n",
    "\n",
    "    #Only if warm adn boostraapp is specified we use the unsupervied labels\n",
    "    if warm == True and boostrap == True:\n",
    "        # initializing main random forest\n",
    "        model_main = randomforest_main()\n",
    "        model_main.fit(X_pool,y_uns_label,sample_weight= y_uns_weight)\n",
    "        print('unsupervised')\n",
    "    #Just Using  the warm true rf but not uns labels\n",
    "    if warm == True and boostrap == False:\n",
    "        # initializing main random forest\n",
    "        model_main = randomforest_main()\n",
    "        model_main.fit(X_train_lb,y_train_lb)\n",
    "    #Normal RF\n",
    "    if warm ==False:\n",
    "        model_main = randomforest_com()\n",
    "        model_main.fit(X_train_lb,y_train_lb)\n",
    "\n",
    "    for clf in members:\n",
    "        # initializing learner\n",
    "        learner = ActiveLearner(\n",
    "            estimator=clf(),\n",
    "            X_training=X_train_lb, y_training=y_train_lb\n",
    "            )\n",
    "        learner_list.append(learner)\n",
    "\n",
    "\n",
    "    # assembling the committee\n",
    "    committee = Committee(learner_list=learner_list,query_strategy=vote_entropy_sampling,boots=boostrap)\n",
    "\n",
    "\n",
    "\n",
    "    # we want to only use the prediction of the random forest which will be incrementally built\n",
    "\n",
    "    #Get predictions of test set\n",
    "    if committee_pred == False:\n",
    "        y_pred = model_main.predict(X_test)\n",
    "    if committee_pred == True:\n",
    "        y_pred = committee.predict(X_test)\n",
    "\n",
    "    #Calculate evaluation metrics\n",
    "    precision_recall_fscore= precision_recall_fscore_support(y_test,y_pred,average='binary',zero_division=0)\n",
    "    precision_scores = [precision_recall_fscore[0]]\n",
    "    recall_scores=  [precision_recall_fscore[1]]\n",
    "    f_score = [precision_recall_fscore[2]]\n",
    "\n",
    "\n",
    "\n",
    "    # query by committee\n",
    "    n_queries = queries\n",
    "    #Active Learning Loop\n",
    "    for idx in range(n_queries):\n",
    "        print('Iteration:',idx)\n",
    "        # Committee models gives the instance to be labeled\n",
    "        query_idx, query_instance = committee.query(X_pool)\n",
    "\n",
    "        if boostrap == True:\n",
    "            #Get the predictions of the most informative instances\n",
    "            preds =committee.predict(X_pool)\n",
    "            #idx_new = np.array([np.argwhere(ids_main == ids[x])[0] for x in query_idx]).squeeze()\n",
    "\n",
    "            #Only chose the instances which disagree with the unsupervised labels\n",
    "            idx_reduced_bool = preds[query_idx] != y_uns_label[query_idx]\n",
    "\n",
    "            if sum(idx_reduced_bool) > 1:\n",
    "                #If there is more than one prediction disagreement chose the first one\n",
    "                idx_reduced = query_idx[idx_reduced_bool][0]\n",
    "                idx_reduced = np.expand_dims(idx_reduced, axis=0)\n",
    "\n",
    "            else:\n",
    "                #If there is not disgreement just chose the first instance which is gotten by vote entropy\n",
    "                print('No disagreement uns labels and pred labels')\n",
    "                idx_reduced = query_idx[0]\n",
    "                idx_reduced = np.expand_dims(idx_reduced, axis=0)\n",
    "        else:\n",
    "            #If no boostrapping use the most informative instance by vote entropy only\n",
    "            idx_reduced = query_idx[0]\n",
    "            idx_reduced = np.expand_dims(idx_reduced, axis=0)\n",
    "\n",
    "\n",
    "        #For AMS data we need to query the user directly\n",
    "        if baseline == False:\n",
    "            #Check if the label has already been given and saved\n",
    "            if np.isnan(y_pool_labels[idx_reduced]):\n",
    "\n",
    "                print(\"Are this two listings the same one? 1-match , 0-nonmatch, 2-more=info\")\n",
    "                ids_int = [int(x) for x in ids[idx_reduced][0][1:-1].split(',')]\n",
    "                print(compare(df_full,ids_int[0],ids_int[1]))\n",
    "                label = int(input())\n",
    "                #If label 2 we need more info\n",
    "                if label == 2:\n",
    "                    print(compare(df_full,ids_int[0],ids_int[1],full=True))\n",
    "                    label = int(input())\n",
    "                    y_new = np.array([label], dtype=float)\n",
    "                else:\n",
    "                    y_new = np.array([label], dtype=float)\n",
    "                new_labels.append([(ids_int[0],ids_int[1]),label])\n",
    "            #If label is already given just take it and dont ask for it again, saving time hopefully\n",
    "            else:\n",
    "                ids_int = [int(x) for x in ids[idx_reduced][0][1:-1].split(',')]\n",
    "                label = y_pool_labels[idx_reduced[0]]\n",
    "                y_new = np.array([label], dtype=float)\n",
    "\n",
    "        if baseline == True:\n",
    "            #For baseline we can access the true labels no need to ask just retreive\n",
    "            label = y_pool_labels[idx_reduced[0]]\n",
    "            y_new = np.array([label], dtype=float)\n",
    "\n",
    "\n",
    "\n",
    "        #This will add the labeled data into the same arrays\n",
    "        X_train_lb = np.concatenate((X_train_lb,X_pool[idx_reduced]),axis=0)\n",
    "        y_train_lb = np.concatenate((y_train_lb,y_new),axis =0)\n",
    "\n",
    "\n",
    "        for model in committee:\n",
    "            #Train the random forest in incremental way with the labeled instance\n",
    "            if incremental_comitee == True:\n",
    "                if type(model.get_params()['estimator'])== sklearn.ensemble._forest.RandomForestClassifier:\n",
    "\n",
    "                    n_estimators = model.get_params()['estimator__n_estimators'] +2\n",
    "                    params_rf = {'estimator__n_estimators':n_estimators} #,'estimator__max_depth': n_estimators\n",
    "                    model.set_params(**params_rf)\n",
    "                    #Teach the random forest , boostrap true is incremental\n",
    "                    model.teach(X_train_lb, y_train_lb)\n",
    "\n",
    "                    #Predict with the new instance\n",
    "                    # we want to only use the prediction of the random forest which will be incrementally built\n",
    "                else:\n",
    "\n",
    "                    model.teach(X_train_lb, y_train_lb)\n",
    "            else:\n",
    "                model.teach(X_train_lb, y_train_lb)\n",
    "\n",
    "        if warm == True and boostrap == True:\n",
    "            #Teach the random forest Increase estimators for incremental learning\n",
    "            model_main.n_estimators += 2\n",
    "            #model_main.fit(X_pool_main,y_uns_label,sample_weight=y_uns_weight)\n",
    "            model_main.fit(X_train_lb,y_train_lb,sample_weight =np.ones(y_train_lb.shape[0]))\n",
    "        if warm == True and boostrap == False:\n",
    "            #Teach the random forest Increase estimators for incremental learning\n",
    "            model_main.n_estimators += 2\n",
    "            #model_main.fit(X_pool_main,y_uns_label,sample_weight=y_uns_weight)\n",
    "            model_main.fit(X_train_lb,y_train_lb)\n",
    "\n",
    "        if warm ==False:\n",
    "            model_main.fit(X_train_lb,y_train_lb)\n",
    "        #Predict with the new instance\n",
    "        # we want to only use the prediction of the random forest which will be incrementally built\n",
    "        if committee_pred == False:\n",
    "            y_pred = model_main.predict(X_test)\n",
    "        else:\n",
    "            y_pred = committee.predict(X_test)\n",
    "\n",
    "        #Get evaluation scores\n",
    "        precision_recall_fscore=precision_recall_fscore_support(y_test,y_pred,average='binary',zero_division=0)\n",
    "        precision_scores.append(precision_recall_fscore[0])\n",
    "        recall_scores.append(precision_recall_fscore[1])\n",
    "        f_score.append(precision_recall_fscore[2])\n",
    "        #Save results for further analysis\n",
    "        dict_results = {'precision_scores':precision_scores,'recall_scores':recall_scores,\n",
    "                        'f_score':f_score}\n",
    "\n",
    "        #Delete the queried instance from the pool\n",
    "        X_pool = np.delete(X_pool, idx_reduced, axis=0)\n",
    "        ids = np.delete(ids, idx_reduced, axis=0)\n",
    "        y_uns_label = np.delete(y_uns_label, idx_reduced, axis=0)\n",
    "        y_pool_labels = np.delete(y_pool_labels, idx_reduced, axis=0)\n",
    "\n",
    "        #Save labels to be added for further runs\n",
    "        if baseline ==False:\n",
    "            new_labels.append([(ids_int[0],ids_int[1]),label])\n",
    "\n",
    "\n",
    "    #Graph to check the learning process\n",
    "    with plt.style.context('seaborn-white'):\n",
    "        plt.figure(figsize=(5, 5))\n",
    "        plt.subplot(1, 1, 1)\n",
    "        plt.title('Precision of your model')\n",
    "        plt.plot(range(n_queries+1), precision_scores)\n",
    "        #plt.scatter(range(n_queries+1), precision_scores)\n",
    "        plt.xlabel('number of queries')\n",
    "        plt.ylabel('Precision')\n",
    "        display.display(plt.gcf())\n",
    "        plt.close('all')\n",
    "    with plt.style.context('seaborn-white'):\n",
    "        plt.figure(figsize=(5, 5))\n",
    "        plt.subplot(1, 1, 1)\n",
    "        plt.title('recall_scores of your model')\n",
    "        plt.plot(range(n_queries+1), recall_scores)\n",
    "        #plt.scatter(range(n_queries+1), recall_scores)\n",
    "        plt.xlabel('number of queries')\n",
    "        plt.ylabel('recall_scores')\n",
    "        display.display(plt.gcf())\n",
    "        plt.close('all')\n",
    "    with plt.style.context('seaborn-white'):\n",
    "        plt.figure(figsize=(5, 5))\n",
    "        plt.subplot(1, 1, 1)\n",
    "        plt.title('f_score of your model')\n",
    "        plt.plot(range(n_queries+1), f_score)\n",
    "        #plt.scatter(range(n_queries+1), f_score)\n",
    "        plt.xlabel('number of queries')\n",
    "        plt.ylabel('f_score')\n",
    "        display.display(plt.gcf())\n",
    "        plt.close('all')\n",
    "    return new_labels, dict_results, model_main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "multi = ['Xl_picture_url_img', 'LaBSE_emb_LaBSE_cos','Descrp_tfidf_tf_idf_cos', 'Lat_Lng_geo','Name_jaro_winker',\n",
    "        'Host_thumbnail_url_jaro_winker', 'Host_thumbnail_url_levenshtein_sim', 'Host_thumbnail_url_jaccard_sim',\n",
    "       'Host_thumbnail_url_relaxed_jaccard_sim','Host_thumbnail_url_overlap_sim', 'Host_thumbnail_url_containment_sim']\n",
    "df_ams_train_no_multi = df_ams_train.drop(multi,axis=1)\n",
    "df_ams_test_no_multi = df_ams_test.drop(multi,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ams_train_im = df_ams_train[['ListingId_1', 'ListingId_2','Xl_picture_url_img', 'ids', 'label', 'agg_score',\n",
    "                               'weights', 'uns_label']]\n",
    "df_ams_test_im = df_ams_test[['ListingId_1', 'ListingId_2','Xl_picture_url_img', 'ids', 'label', 'agg_score',\n",
    "                               'weights', 'uns_label']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Recalculate the uns scores after taking out the test set\n",
    "df_ams_train_im =df_ams_train_im.drop(['agg_score','weights','uns_label'],axis=1)\n",
    "sorted_dataset = scoring_unsupervised(df_ams_train_im,baseline=False)\n",
    "threshold,index = elbow_threshold(sorted_dataset) \n",
    "matches_score_weight, nonmatches_score_weight = unsupervised_labels(sorted_dataset,threshold)\n",
    "columns = ['ids','agg_score','weights','uns_label']\n",
    "uns_labels = matches_score_weight+nonmatches_score_weight\n",
    "df_uns = pd.DataFrame(uns_labels,columns=columns)\n",
    "df_ams_train_im= df_ams_train_im.merge(df_uns,on='ids')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_results_full = []\n",
    "for i in range(5):\n",
    "    orcl_labels,im_results_full = al(df_ams_train_im,df_ams_test_im,queries=30,\n",
    "                                            baseline=False,committee_pred= False,\n",
    "                                            incremental_comitee = False,boostrap = True,\n",
    "                                            warm= True)\n",
    "    \n",
    "    img_results_full.append(im_results_full)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_name = \"img_results_full.pkl\"\n",
    "\n",
    "open_file = open(file_name, \"wb\")\n",
    "pickle.dump(img_results_full, open_file)\n",
    "open_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ams_train_des = df_ams_train[['ListingId_1', 'ListingId_2','LaBSE_emb_LaBSE_cos','Descrp_tfidf_tf_idf_cos', 'ids', 'label', 'agg_score',\n",
    "                               'weights', 'uns_label']]\n",
    "df_ams_test_des = df_ams_test[['ListingId_1', 'ListingId_2','LaBSE_emb_LaBSE_cos','Descrp_tfidf_tf_idf_cos', 'ids', 'label', 'agg_score',\n",
    "                               'weights', 'uns_label']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp_results_full = []\n",
    "for i in range(5):\n",
    "    orcl_labels,labse_results_full = al(df_ams_train_des,df_ams_test_des,queries=30,\n",
    "                                            baseline=False,committee_pred= False,\n",
    "                                            incremental_comitee = False,boostrap = True,\n",
    "                                            warm= True)\n",
    "    \n",
    "    nlp_results_full.append(labse_results_full)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_name = \"nlp_results_full.pkl\"\n",
    "\n",
    "open_file = open(file_name, \"wb\")\n",
    "pickle.dump(nlp_results_full, open_file)\n",
    "open_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "manual_labeled = []\n",
    "\n",
    "for (tuple_1,tuple_2),label in orcl_labels:\n",
    "    if label ==2 :\n",
    "        manual_labeled.append([str((tuple_1,tuple_2)),np.float('nan')])\n",
    "        manual_labeled.append([str((tuple_2,tuple_1)),np.float('nan')])\n",
    "    else:    \n",
    "        manual_labeled.append([str((tuple_1,tuple_2)),label])\n",
    "        manual_labeled.append([str((tuple_2,tuple_1)),label])\n",
    "\n",
    "#Save the labeled data into the main dataframe\n",
    "for tuples,label in manual_labeled:\n",
    "    df_ams_train['label'][df_ams_train['ids']== tuples] = label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "orcl_labels,no_modal_results_full = al(df_ams_train_no_multi,df_ams_test_no_multi,queries=30,\n",
    "                                        baseline=False,committee_pred= False,\n",
    "                                        incremental_comitee = False,boostrap = True,\n",
    "                                        warm= True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "orcl_labels,no_modal_results_full_1 = al(df_ams_train_no_multi,df_ams_test_no_multi,queries=30,\n",
    "                                        baseline=False,committee_pred= False,\n",
    "                                        incremental_comitee = False,boostrap = True,\n",
    "                                        warm= True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "orcl_labels,no_modal_results_full_2 = al(df_ams_train_no_multi,df_ams_test_no_multi,queries=30,\n",
    "                                        baseline=False,committee_pred= False,\n",
    "                                        incremental_comitee = False,boostrap = True,\n",
    "                                        warm= True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "orcl_labels,no_modal_results_full_3 = al(df_ams_train_no_multi,df_ams_test_no_multi,queries=30,\n",
    "                                        baseline=False,committee_pred= False,\n",
    "                                        incremental_comitee = False,boostrap = True,\n",
    "                                        warm= True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "orcl_labels,no_modal_results_full_4 = al(df_ams_train_no_multi,df_ams_test_no_multi,queries=30,\n",
    "                                        baseline=False,committee_pred= False,\n",
    "                                        incremental_comitee = False,boostrap = True,\n",
    "                                        warm= True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "orcl_labels,no_modal_results_full_5 = al(df_ams_train_no_multi,df_ams_test_no_multi,queries=30,\n",
    "                                        baseline=False,committee_pred= False,\n",
    "                                        incremental_comitee = False,boostrap = True,\n",
    "                                        warm= True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "no_modal_results_full = [no_modal_results_full,no_modal_results_full_1,no_modal_results_full_2,\n",
    "                         no_modal_results_full_5,no_modal_results_full_4,no_modal_results_full_3]\n",
    "\n",
    "file_name = \"no_modal_results_full.pkl\"\n",
    "\n",
    "open_file = open(file_name, \"wb\")\n",
    "pickle.dump(no_modal_results_full, open_file)\n",
    "open_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "orcl_labels,ams_results_full = al(df_ams_train,df_ams_test,queries=30,\n",
    "                                        baseline=False,committee_pred= False,\n",
    "                                        incremental_comitee = False,boostrap = True,\n",
    "                                        warm= True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Take out the labeled data for making a test set \n",
    "manual_labeled = []\n",
    "\n",
    "for (tuple_1,tuple_2),label in orcl_labels:\n",
    "    if label ==2 :\n",
    "        manual_labeled.append([str((tuple_1,tuple_2)),np.float('nan')])\n",
    "        manual_labeled.append([str((tuple_2,tuple_1)),np.float('nan')])\n",
    "    else:    \n",
    "        manual_labeled.append([str((tuple_1,tuple_2)),label])\n",
    "        manual_labeled.append([str((tuple_2,tuple_1)),label])\n",
    "\n",
    "#Save the labeled data into the main dataframe\n",
    "for tuples,label in manual_labeled:\n",
    "    df_ams_train['label'][df_ams_train['ids']== tuples] = label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "orcl_labels,ams_results_full_1 = al(df_ams_train,df_ams_test,queries=30,\n",
    "                                        baseline=False,committee_pred= False,\n",
    "                                        incremental_comitee = False,boostrap = True,\n",
    "                                        warm= True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Take out the labeled data for making a test set \n",
    "manual_labeled = []\n",
    "\n",
    "for (tuple_1,tuple_2),label in orcl_labels:\n",
    "    if label ==2 :\n",
    "        manual_labeled.append([str((tuple_1,tuple_2)),np.float('nan')])\n",
    "        manual_labeled.append([str((tuple_2,tuple_1)),np.float('nan')])\n",
    "    else:    \n",
    "        manual_labeled.append([str((tuple_1,tuple_2)),label])\n",
    "        manual_labeled.append([str((tuple_2,tuple_1)),label])\n",
    "\n",
    "#Save the labeled data into the main dataframe\n",
    "for tuples,label in manual_labeled:\n",
    "    df_ams_train['label'][df_ams_train['ids']== tuples] = label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "orcl_labels,ams_results_full_2 = al(df_ams_train,df_ams_test,queries=30,\n",
    "                                        baseline=False,committee_pred= False,\n",
    "                                        incremental_comitee = False,boostrap = True,\n",
    "                                        warm= True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Take out the labeled data for making a test set \n",
    "manual_labeled = []\n",
    "\n",
    "for (tuple_1,tuple_2),label in orcl_labels:\n",
    "    if label ==2 :\n",
    "        manual_labeled.append([str((tuple_1,tuple_2)),np.float('nan')])\n",
    "        manual_labeled.append([str((tuple_2,tuple_1)),np.float('nan')])\n",
    "    else:    \n",
    "        manual_labeled.append([str((tuple_1,tuple_2)),label])\n",
    "        manual_labeled.append([str((tuple_2,tuple_1)),label])\n",
    "\n",
    "#Save the labeled data into the main dataframe\n",
    "for tuples,label in manual_labeled:\n",
    "    df_ams_train['label'][df_ams_train['ids']== tuples] = label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "orcl_labels,ams_results_full_3 = al(df_ams_train,df_ams_test,queries=30,\n",
    "                                        baseline=False,committee_pred= False,\n",
    "                                        incremental_comitee = False,boostrap = True,\n",
    "                                        warm= True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Take out the labeled data for making a test set \n",
    "manual_labeled = []\n",
    "\n",
    "for (tuple_1,tuple_2),label in orcl_labels:\n",
    "    if label ==2 :\n",
    "        manual_labeled.append([str((tuple_1,tuple_2)),np.float('nan')])\n",
    "        manual_labeled.append([str((tuple_2,tuple_1)),np.float('nan')])\n",
    "    else:    \n",
    "        manual_labeled.append([str((tuple_1,tuple_2)),label])\n",
    "        manual_labeled.append([str((tuple_2,tuple_1)),label])\n",
    "\n",
    "#Save the labeled data into the main dataframe\n",
    "for tuples,label in manual_labeled:\n",
    "    df_ams_train['label'][df_ams_train['ids']== tuples] = label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "orcl_labels,ams_results_full_4 = al(df_ams_train,df_ams_test,queries=30,\n",
    "                                        baseline=False,committee_pred= False,\n",
    "                                        incremental_comitee = False,boostrap = True,\n",
    "                                        warm= True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Take out the labeled data for making a test set \n",
    "manual_labeled = []\n",
    "\n",
    "for (tuple_1,tuple_2),label in orcl_labels:\n",
    "    if label ==2 :\n",
    "        manual_labeled.append([str((tuple_1,tuple_2)),np.float('nan')])\n",
    "        manual_labeled.append([str((tuple_2,tuple_1)),np.float('nan')])\n",
    "    else:    \n",
    "        manual_labeled.append([str((tuple_1,tuple_2)),label])\n",
    "        manual_labeled.append([str((tuple_2,tuple_1)),label])\n",
    "\n",
    "#Save the labeled data into the main dataframe\n",
    "for tuples,label in manual_labeled:\n",
    "    df_ams_train['label'][df_ams_train['ids']== tuples] = label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resuts_full_ams = [ams_results_full,ams_results_full_1,ams_results_full_2,ams_results_full_3,ams_results_full_4]\n",
    "\n",
    "file_name = \"resuts_full_rf_ams_red.pkl\"\n",
    "\n",
    "open_file = open(file_name, \"wb\")\n",
    "pickle.dump(resuts_full_ams, open_file)\n",
    "open_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#len(df_ams_train.label.dropna())\n",
    "df_ams_train.to_csv('df_ams_train_nh_1.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "orcl_labels,ams_results_main_no_boot = al(df_ams_train,df_ams_test,queries=30,\n",
    "                                        baseline=False,committee_pred= False,\n",
    "                                        incremental_comitee = False,boostrap = False,\n",
    "                                        warm= True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "orcl_labels,ams_results_main_no_boot_1 = al(df_ams_train,df_ams_test,queries=30,\n",
    "                                        baseline=False,committee_pred= False,\n",
    "                                        incremental_comitee = False,boostrap = False,\n",
    "                                        warm= True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "orcl_labels,ams_results_main_no_boot_2 = al(df_ams_train,df_ams_test,queries=30,\n",
    "                                        baseline=False,committee_pred= False,\n",
    "                                        incremental_comitee = False,boostrap = False,\n",
    "                                        warm= True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "orcl_labels,ams_results_main_no_boot_3 = al(df_ams_train,df_ams_test,queries=30,\n",
    "                                        baseline=False,committee_pred= False,\n",
    "                                        incremental_comitee = False,boostrap = False,\n",
    "                                        warm= True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "orcl_labels,ams_results_main_no_boot_4 = al(df_ams_train,df_ams_test,queries=30,\n",
    "                                        baseline=False,committee_pred= False,\n",
    "                                        incremental_comitee = False,boostrap = False,\n",
    "                                        warm= True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resuts_noboot_ams = [ams_results_main_no_boot_4,ams_results_main_no_boot_3,ams_results_main_no_boot_2\n",
    "                     ,ams_results_main_no_boot_1,ams_results_main_no_boot]\n",
    "\n",
    "file_name = \"resuts_noboot_ams.pkl\"\n",
    "\n",
    "open_file = open(file_name, \"wb\")\n",
    "pickle.dump(resuts_noboot_ams, open_file)\n",
    "open_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Take out the labeled data for making a test set \n",
    "manual_labeled = []\n",
    "\n",
    "for (tuple_1,tuple_2),label in orcl_labels:\n",
    "    if label ==2 :\n",
    "        manual_labeled.append([str((tuple_1,tuple_2)),np.float('nan')])\n",
    "        manual_labeled.append([str((tuple_2,tuple_1)),np.float('nan')])\n",
    "    else:    \n",
    "        manual_labeled.append([str((tuple_1,tuple_2)),label])\n",
    "        manual_labeled.append([str((tuple_2,tuple_1)),label])\n",
    "\n",
    "#Save the labeled data into the main dataframe\n",
    "for tuples,label in manual_labeled:\n",
    "    df_ams_train['label'][df_ams_train['ids']== tuples] = label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "orcl_labels,ams_results_main_no_boot_no_warm = al(df_ams_train,df_ams_test,queries=30,\n",
    "                                        baseline=False,committee_pred= False,\n",
    "                                        incremental_comitee = False,boostrap = False,\n",
    "                                        warm= False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "orcl_labels,ams_results_main_no_boot_no_warm_1 = al(df_ams_train,df_ams_test,queries=30,\n",
    "                                        baseline=False,committee_pred= False,\n",
    "                                        incremental_comitee = False,boostrap = False,\n",
    "                                        warm= False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "orcl_labels,ams_results_main_no_boot_no_warm_2 = al(df_ams_train,df_ams_test,queries=30,\n",
    "                                        baseline=False,committee_pred= False,\n",
    "                                        incremental_comitee = False,boostrap = False,\n",
    "                                        warm= False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "orcl_labels,ams_results_main_no_boot_no_warm_3 = al(df_ams_train,df_ams_test,queries=30,\n",
    "                                        baseline=False,committee_pred= False,\n",
    "                                        incremental_comitee = False,boostrap = False,\n",
    "                                        warm= False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "orcl_labels,ams_results_main_no_boot_no_warm_4 = al(df_ams_train,df_ams_test,queries=30,\n",
    "                                        baseline=False,committee_pred= False,\n",
    "                                        incremental_comitee = False,boostrap = False,\n",
    "                                        warm= False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Take out the labeled data for making a test set \n",
    "manual_labeled = []\n",
    "\n",
    "for (tuple_1,tuple_2),label in orcl_labels:\n",
    "    if label ==2 :\n",
    "        manual_labeled.append([str((tuple_1,tuple_2)),np.float('nan')])\n",
    "        manual_labeled.append([str((tuple_2,tuple_1)),np.float('nan')])\n",
    "    else:    \n",
    "        manual_labeled.append([str((tuple_1,tuple_2)),label])\n",
    "        manual_labeled.append([str((tuple_2,tuple_1)),label])\n",
    "\n",
    "#Save the labeled data into the main dataframe\n",
    "for tuples,label in manual_labeled:\n",
    "    df_ams_train['label'][df_ams_train['ids']== tuples] = label\n",
    "df_ams_train.to_csv('df_ams_train_nh_1.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resuts_no_boot_no_warm_ams = [ams_results_main_no_boot_no_warm_4,ams_results_main_no_boot_no_warm_3,ams_results_main_no_boot_no_warm_2\n",
    "                     ,ams_results_main_no_boot_no_warm_1,ams_results_main_no_boot_no_warm]\n",
    "\n",
    "file_name = \"resuts_no_boot_no_warm_ams.pkl\"\n",
    "\n",
    "open_file = open(file_name, \"wb\")\n",
    "pickle.dump(resuts_no_boot_no_warm_ams, open_file)\n",
    "open_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create an average of the 5 runs of the active learning set up \n",
    "precision_full_rf = []\n",
    "recall_full_rf = []\n",
    "f_score_full_rf = []\n",
    "\n",
    "precision_no_boot_rf = []\n",
    "recall_no_boot_rf = []\n",
    "f_score_no_boot_rf = []\n",
    "\n",
    "precision_rf_noboot_nowarm = []\n",
    "recall_rf_noboot_nowarm = []\n",
    "f_score_rf_noboot_nowarm = []\n",
    "\n",
    "for dicts in resuts_full_ams:\n",
    "    precision_full_rf.append(dicts['precision_scores'])\n",
    "    recall_full_rf.append(dicts['recall_scores'])\n",
    "    f_score_full_rf.append(dicts['f_score'])\n",
    "\n",
    "precision_full_rf_mean =np.mean(precision_full_rf,axis=0)\n",
    "recall_full_rf_mean =np.mean(recall_full_rf,axis=0)\n",
    "f_score_full_rf_mean = np.mean(f_score_full_rf,axis=0)\n",
    "\n",
    "precision_full_rf_sd =np.std(precision_full_rf,axis=0)\n",
    "recall_full_rf_sd =np.std(recall_full_rf,axis=0)\n",
    "f_score_full_rf_sd = np.std(f_score_full_rf,axis=0)\n",
    "\n",
    "\n",
    "for dicts in resuts_noboot_ams:\n",
    "    precision_no_boot_rf.append(dicts['precision_scores'])\n",
    "    recall_no_boot_rf.append(dicts['recall_scores'])\n",
    "    f_score_no_boot_rf.append(dicts['f_score'])\n",
    "\n",
    "precision_no_boot_rf_mean =np.mean(precision_no_boot_rf,axis=0)\n",
    "recall_no_boot_rf_mean =np.mean(recall_no_boot_rf,axis=0)\n",
    "f_score_no_boot_rf_mean = np.mean(f_score_no_boot_rf,axis=0)\n",
    "\n",
    "precision_no_boot_rf_sd =np.std(precision_no_boot_rf,axis=0)\n",
    "recall_no_boot_rf_sd =np.std(recall_no_boot_rf,axis=0)\n",
    "f_score_no_boot_rf_sd = np.std(f_score_no_boot_rf,axis=0)\n",
    "\n",
    "for dicts in resuts_no_boot_no_warm_ams:\n",
    "    precision_rf_noboot_nowarm.append(dicts['precision_scores'])\n",
    "    recall_rf_noboot_nowarm.append(dicts['recall_scores'])\n",
    "    f_score_rf_noboot_nowarm.append(dicts['f_score'])\n",
    "\n",
    "precision_rf_noboot_nowarm_mean =np.mean(precision_rf_noboot_nowarm,axis=0)\n",
    "recall_rf_noboot_nowarm_mean =np.mean(recall_rf_noboot_nowarm,axis=0)\n",
    "f_score_rf_noboot_nowarm_mean = np.mean(f_score_rf_noboot_nowarm,axis=0)\n",
    "\n",
    "precision_rf_noboot_nowarm_sd =np.std(precision_rf_noboot_nowarm,axis=0)\n",
    "recall_rf_noboot_nowarm_mean =np.std(recall_rf_noboot_nowarm,axis=0)\n",
    "f_score_rf_noboot_nowarm_sd = np.std(f_score_rf_noboot_nowarm,axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "fig, ax = plt.subplots(1)\n",
    "fig.set_size_inches(10,8)\n",
    "ax.plot(np.arange(31), f_score_full_rf_mean, lw=1, label='boot rf', color='blue')\n",
    "\n",
    "ax.plot(np.arange(31), f_score_no_boot_rf_mean, lw=1, label=' no boot rf', color='green')\n",
    "ax.plot(np.arange(31), f_score_rf_noboot_nowarm_mean, lw=1, label='no boot no warm rf', color='purple')\n",
    "#ax.plot([0, 31], [0.7959183673469387,0.7959183673469387], 'k-', lw=2)\n",
    "ax.fill_between(np.arange(31), f_score_full_rf_mean + f_score_full_rf_sd, f_score_full_rf_mean-f_score_full_rf_sd\n",
    "                ,facecolor='blue', alpha=0.2)\n",
    "\n",
    "ax.fill_between(np.arange(31), f_score_no_boot_rf_mean + f_score_no_boot_rf_sd, f_score_no_boot_rf_mean-f_score_no_boot_rf_sd\n",
    "                ,facecolor='green', alpha=0.1)\n",
    "ax.fill_between(np.arange(31), f_score_rf_noboot_nowarm_mean + f_score_rf_noboot_nowarm_sd, f_score_rf_noboot_nowarm_mean-f_score_rf_noboot_nowarm_sd\n",
    "                ,facecolor='purple', alpha=0.1)                                \n",
    "ax.set_title('F score')\n",
    "ax.legend(loc='lower right')\n",
    "ax.set_xlabel('Iterations')\n",
    "ax.set_ylabel('F_score')\n",
    "\n",
    "ax.grid()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_,dict_results_com = al(df_base_train,df_test = df_base_test ,queries=1,\n",
    "                                        baseline=True,committee_pred= True,\n",
    "                                        incremental_comitee = True,boostrap = True,\n",
    "                                        warm= True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resuts_full_comitte = []\n",
    "for i in range(5):    \n",
    "    _,dict_results_com = al(df_base_train,df_test = df_base_test ,queries=100,\n",
    "                                        baseline=True,committee_pred= True,\n",
    "                                        incremental_comitee = True,boostrap = True,\n",
    "                                        warm= True)\n",
    "    resuts_full_comitte.append(dict_results_com)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_name = \"resuts_full_comitte.pkl\"\n",
    "\n",
    "open_file = open(file_name, \"wb\")\n",
    "pickle.dump(resuts_full_comitte, open_file)\n",
    "open_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resuts_full_rf = []\n",
    "for i in range(5):   \n",
    "    _,dict_results_main = al(df_base_train,df_test = df_base_test ,queries=100,\n",
    "                                        baseline=True,committee_pred= False,\n",
    "                                        incremental_comitee = False,boostrap = True,\n",
    "                                        warm=True)\n",
    "    resuts_full_rf.append(dict_results_main)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_name = \"resuts_full_rf.pkl\"\n",
    "\n",
    "open_file = open(file_name, \"wb\")\n",
    "pickle.dump(resuts_full_rf, open_file)\n",
    "open_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resuts_rf_no_boot = []\n",
    "for i in range(5): \n",
    "    _,dict_results_main_no_boot = al(df_base_train,df_test = df_base_test ,queries=100,\n",
    "                                        baseline=True,committee_pred= False,\n",
    "                                        incremental_comitee = False,boostrap = False,\n",
    "                                        warm=True)\n",
    "    resuts_rf_no_boot.append(dict_results_main_no_boot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_name = \"resuts_rf_no_boot.pkl\"\n",
    "\n",
    "open_file = open(file_name, \"wb\")\n",
    "pickle.dump(resuts_rf_no_boot, open_file)\n",
    "open_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resuts_rf_noboot_nowarm = []\n",
    "for i in range(5):\n",
    "    _,dict_results_main_no_boot_no_warm = al(df_base_train,df_test = df_base_test ,queries=100,\n",
    "                                        baseline=True,committee_pred= False,\n",
    "                                        incremental_comitee = False,boostrap=False,\n",
    "                                        warm=False)\n",
    "    resuts_rf_noboot_nowarm.append(dict_results_main_no_boot_no_warm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_name = \"resuts_rf_noboot_nowarm.pkl\"\n",
    "\n",
    "open_file = open(file_name, \"wb\")\n",
    "pickle.dump(resuts_rf_noboot_nowarm, open_file)\n",
    "open_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_name = 'resuts_full_rf.pkl'\n",
    "open_file = open(file_name, \"rb\")\n",
    "resuts_full_rf = pickle.load(open_file)\n",
    "open_file.close()\n",
    "\n",
    "file_name = 'resuts_full_comitte.pkl'\n",
    "open_file = open(file_name, \"rb\")\n",
    "resuts_full_comitte = pickle.load(open_file)\n",
    "open_file.close()\n",
    "\n",
    "file_name = 'resuts_rf_no_boot.pkl'\n",
    "open_file = open(file_name, \"rb\")\n",
    "resuts_rf_no_boot = pickle.load(open_file)\n",
    "open_file.close()\n",
    "\n",
    "file_name = 'resuts_rf_noboot_nowarm.pkl'\n",
    "open_file = open(file_name, \"rb\")\n",
    "resuts_noboot_nowarm = pickle.load(open_file)\n",
    "open_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create an average of the 5 runs of the active learning set up \n",
    "precision_full_rf = []\n",
    "recall_full_rf = []\n",
    "f_score_full_rf = []\n",
    "\n",
    "precision_full_cmt = []\n",
    "recall_full_cmt = []\n",
    "f_score_full_cmt = []\n",
    "\n",
    "precision_no_boot_rf = []\n",
    "recall_no_boot_rf = []\n",
    "f_score_no_boot_rf = []\n",
    "\n",
    "precision_rf_noboot_nowarm = []\n",
    "recall_rf_noboot_nowarm = []\n",
    "f_score_rf_noboot_nowarm = []\n",
    "\n",
    "for dicts in resuts_full_rf:\n",
    "    precision_full_rf.append(dicts['precision_scores'])\n",
    "    recall_full_rf.append(dicts['recall_scores'])\n",
    "    f_score_full_rf.append(dicts['f_score'])\n",
    "\n",
    "precision_full_rf_mean =np.mean(precision_full_rf,axis=0)\n",
    "recall_full_rf_mean =np.mean(recall_full_rf,axis=0)\n",
    "f_score_full_rf_mean = np.mean(f_score_full_rf,axis=0)\n",
    "\n",
    "precision_full_rf_sd =np.std(precision_full_rf,axis=0)\n",
    "recall_full_rf_sd =np.std(recall_full_rf,axis=0)\n",
    "f_score_full_rf_sd = np.std(f_score_full_rf,axis=0)\n",
    "\n",
    "for dicts in resuts_full_comitte:\n",
    "    precision_full_cmt.append(dicts['precision_scores'])\n",
    "    recall_full_cmt.append(dicts['recall_scores'])\n",
    "    f_score_full_cmt.append(dicts['f_score'])\n",
    "\n",
    "precision_full_cmt_mean =np.mean(precision_full_cmt,axis=0)\n",
    "recall_full_cmt_mean =np.mean(recall_full_cmt,axis=0)\n",
    "f_score_full_cmt_mean = np.mean(f_score_full_cmt,axis=0)\n",
    "\n",
    "precision_full_cmt_sd =np.std(precision_full_cmt,axis=0)\n",
    "recall_full_cmt_sd =np.std(recall_full_cmt,axis=0)\n",
    "f_score_full_cmt_sd = np.std(f_score_full_cmt,axis=0)\n",
    "\n",
    "for dicts in resuts_rf_no_boot:\n",
    "    precision_no_boot_rf.append(dicts['precision_scores'])\n",
    "    recall_no_boot_rf.append(dicts['recall_scores'])\n",
    "    f_score_no_boot_rf.append(dicts['f_score'])\n",
    "\n",
    "precision_no_boot_rf_mean =np.mean(precision_no_boot_rf,axis=0)\n",
    "recall_no_boot_rf_mean =np.mean(recall_no_boot_rf,axis=0)\n",
    "f_score_no_boot_rf_mean = np.mean(f_score_no_boot_rf,axis=0)\n",
    "\n",
    "precision_no_boot_rf_sd =np.std(precision_no_boot_rf,axis=0)\n",
    "recall_no_boot_rf_sd =np.std(recall_no_boot_rf,axis=0)\n",
    "f_score_no_boot_rf_sd = np.std(f_score_no_boot_rf,axis=0)\n",
    "\n",
    "for dicts in resuts_rf_noboot_nowarm:\n",
    "    precision_rf_noboot_nowarm.append(dicts['precision_scores'])\n",
    "    recall_rf_noboot_nowarm.append(dicts['recall_scores'])\n",
    "    f_score_rf_noboot_nowarm.append(dicts['f_score'])\n",
    "\n",
    "precision_rf_noboot_nowarm_mean =np.mean(precision_rf_noboot_nowarm,axis=0)\n",
    "recall_rf_noboot_nowarm_mean =np.mean(recall_rf_noboot_nowarm,axis=0)\n",
    "f_score_rf_noboot_nowarm_mean = np.mean(f_score_rf_noboot_nowarm,axis=0)\n",
    "\n",
    "precision_rf_noboot_nowarm_sd =np.std(precision_rf_noboot_nowarm,axis=0)\n",
    "recall_rf_noboot_nowarm_mean =np.std(recall_rf_noboot_nowarm,axis=0)\n",
    "f_score_rf_noboot_nowarm_sd = np.std(f_score_rf_noboot_nowarm,axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.pyplot import figure\n",
    "fig, ax = plt.subplots(1)\n",
    "fig.set_size_inches(10,8)\n",
    "ax.plot(np.arange(101), f_score_full_rf_mean, lw=1, label='boot rf', color='blue')\n",
    "#ax.plot(np.arange(101), f_score_full_cmt_mean, lw=1, label='boot committee', color='red')\n",
    "ax.plot(np.arange(101), f_score_no_boot_rf_mean, lw=1, label=' no boot rf', color='green')\n",
    "ax.plot(np.arange(101), f_score_rf_noboot_nowarm_mean, lw=1, label='no boot no warm rf', color='purple')\n",
    "ax.plot([0, 101], [0.7959183673469387,0.7959183673469387], 'k-', lw=2)\n",
    "ax.fill_between(np.arange(101), f_score_full_rf_mean + f_score_full_rf_sd, f_score_full_rf_mean-f_score_full_rf_sd\n",
    "                ,facecolor='blue', alpha=0.2)\n",
    "#ax.fill_between(np.arange(101), f_score_full_cmt_mean + f_score_full_cmt_sd, f_score_full_cmt_mean-f_score_full_cmt_sd\n",
    "#                ,facecolor='red', alpha=0.2)\n",
    "ax.fill_between(np.arange(101), f_score_no_boot_rf_mean + f_score_no_boot_rf_sd, f_score_no_boot_rf_mean-f_score_no_boot_rf_sd\n",
    "                ,facecolor='green', alpha=0.1)\n",
    "ax.fill_between(np.arange(101), f_score_rf_noboot_nowarm_mean + f_score_rf_noboot_nowarm_sd, f_score_rf_noboot_nowarm_mean-f_score_rf_noboot_nowarm_sd\n",
    "                ,facecolor='purple', alpha=0.1)                                \n",
    "ax.set_title('F score')\n",
    "ax.legend(loc='lower right')\n",
    "ax.set_xlabel('Iterations')\n",
    "ax.set_ylabel('F_score')\n",
    "\n",
    "ax.grid()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Labeled data\n",
    "open_file = open('labels-all.pkl', \"rb\")\n",
    "labels_test = pickle.load(open_file)\n",
    "open_file.close()\n",
    "\n",
    "manual_labeled = []\n",
    "\n",
    "for (tuple_1,tuple_2),label in labels_test:\n",
    "    if label ==2 :\n",
    "        manual_labeled.append([str((tuple_1,tuple_2)),np.float('nan')])\n",
    "        manual_labeled.append([str((tuple_2,tuple_1)),np.float('nan')])\n",
    "    else:    \n",
    "        manual_labeled.append([str((tuple_1,tuple_2)),label])\n",
    "        manual_labeled.append([str((tuple_2,tuple_1)),label])\n",
    "\n",
    " #Merging to have it with the full data   \n",
    "df_labeled = pd.DataFrame(manual_labeled,columns=['ids','labels'])\n",
    "df_labeled[df_labeled['labels']==1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compare(df_full,10908624, 10908624,full=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option(\"display.max_rows\", None, \"display.max_columns\", None)\n",
    "\n",
    "pd.concat([df_full[df_full['ListingId']==19184032],df_full[df_full['ListingId']==12058561]]).transpose().dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x =33359533\n",
    "#compare(df_full,18113908, 10175956,full=True)\n",
    "12058561\n",
    "compare(df_full,19184032,12058561,full=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_uns[(df_uns['ListingId_1']==19184032) & (df_uns['ListingId_2']==12058561)].transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_uns[df_uns['Xl_picture_url_img']==1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "_,_,model = al(df_ams_train,df_ams_test,queries=30,\n",
    "                                        baseline=False,committee_pred= False,\n",
    "                                        incremental_comitee = False,boostrap = True,\n",
    "                                        warm= True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "not_train = ['ListingId_1','ListingId_2','ids', 'agg_score', 'weights','uns_label','label']\n",
    "X_pool = df_ams_train.drop(not_train, axis=1).values\n",
    "labels_pred = model.predict(X_pool)\n",
    "df_ams_train['pred_label'] = labels_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final = df_ams_train[['ListingId_1','ListingId_2','ids','uns_label','label','pred_label']]\n",
    "df_final.to_csv('df_final.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "bc9e0a135ba6f5ed53de0042d65bc7b4266590385bf12c893a972c5e28ec494f"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
